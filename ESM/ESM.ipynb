{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset\n",
    "\n",
    "!mkdir TAPE_benchmark\n",
    "%cd TAPE_benchmark\n",
    "!git clone https://github.com/songlab-cal/tape.git\n",
    "!wget http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/stability.tar.gz\n",
    "!tar -xf stability.tar.gz\n",
    "\n",
    "\n",
    "#import the libraries\n",
    "\n",
    "from TAPE_benchmark.tape.tape.datasets import *\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset and create the dataloaders\n",
    "\n",
    "Use the esm2's tokenizer to create dataset and dataloader that fit esm2's model & token format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<cls>': 0,\n",
       " '<pad>': 1,\n",
       " '<eos>': 2,\n",
       " '<unk>': 3,\n",
       " 'L': 4,\n",
       " 'A': 5,\n",
       " 'G': 6,\n",
       " 'V': 7,\n",
       " 'S': 8,\n",
       " 'E': 9,\n",
       " 'R': 10,\n",
       " 'T': 11,\n",
       " 'I': 12,\n",
       " 'D': 13,\n",
       " 'P': 14,\n",
       " 'K': 15,\n",
       " 'Q': 16,\n",
       " 'N': 17,\n",
       " 'F': 18,\n",
       " 'Y': 19,\n",
       " 'M': 20,\n",
       " 'H': 21,\n",
       " 'W': 22,\n",
       " 'C': 23,\n",
       " 'X': 24,\n",
       " 'B': 25,\n",
       " 'U': 26,\n",
       " 'Z': 27,\n",
       " 'O': 28,\n",
       " '.': 29,\n",
       " '-': 30,\n",
       " '<null_1>': 31,\n",
       " '<mask>': 32}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lmdb_dataset = dataset_factory('./TAPE_benchmark/stability/stability_test.lmdb', in_memory=False)\n",
    "val_lmdb_dataset = dataset_factory('./TAPE_benchmark/stability/stability_valid.lmdb', in_memory=False)\n",
    "train_lmdb_dataset = dataset_factory('./TAPE_benchmark/stability/stability_train.lmdb', in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in train: 53614, validation: 2512, test: 12851\n"
     ]
    }
   ],
   "source": [
    "print(f'number of samples in train: {len(train_lmdb_dataset)}, validation: {len(val_lmdb_dataset)}, test: {len(test_lmdb_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dataset for the esm model\n",
    "\n",
    "class StabilityDataset(Dataset):\n",
    "    def __init__(self, LMDB_dataset, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = LMDB_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataset[index]\n",
    "        token_ids = np.array(self.tokenizer.encode(item['primary']))\n",
    "        input_mask = np.ones_like(token_ids)\n",
    "        stability_score = float(item['stability_score'])\n",
    "        return token_ids, input_mask, stability_score\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    token_ids, input_mask, stability_score = tuple(zip(*batch)) \n",
    "    token_ids = torch.from_numpy(pad_sequences(np.array(token_ids), 0))\n",
    "    input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
    "    stability_true_value = torch.FloatTensor(stability_score) \n",
    "    stability_true_value = stability_true_value.unsqueeze(1)\n",
    "\n",
    "    return {'input_ids': token_ids,\n",
    "            'input_mask': input_mask,\n",
    "            'targets': stability_true_value}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = StabilityDataset(train_lmdb_dataset, tokenizer)\n",
    "val_dataset = StabilityDataset(val_lmdb_dataset, tokenizer)\n",
    "test_dataset = StabilityDataset(test_lmdb_dataset, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "BATCHSIZE = 64\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=BATCHSIZE,collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=BATCHSIZE,collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=BATCHSIZE,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the pretrained model and define the model\n",
    "\n",
    "ESM2 model: Evolutionary Scale Modeling \n",
    "- Transformer protein language models\n",
    "- This model was trained on masked amino acid sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmModel(\n",
       "  (embeddings): EsmEmbeddings(\n",
       "    (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
       "  )\n",
       "  (encoder): EsmEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): EsmPooler(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (contact_head): EsmContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrainedESM = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "pretrainedESM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the fine tuning model\n",
    "\n",
    "class StabilityPredictor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 esm: nn.Module,\n",
    "                 enc_hid_dim=320, \n",
    "                 outputs=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.esm = esm\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.pre_predictor = nn.Linear(self.enc_hid_dim, self.enc_hid_dim)\n",
    "        self.predictor = nn.Linear(self.enc_hid_dim, outputs)\n",
    "        \n",
    "    def forward(self,\n",
    "                seq):\n",
    "        esm_output = self.esm(seq)\n",
    "        last_hidden_state = esm_output.last_hidden_state ##(batch, seq_len, dim)\n",
    "        pooled_output = last_hidden_state[:,0,:] ##(batch, dim)\n",
    "        pooled_output = self.pre_predictor(pooled_output)\n",
    "        stability = self.predictor(pooled_output) ## (batch, num_labels)\n",
    "\n",
    "        return stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_classification_head_weights(m: nn.Module, hidden_size=320):\n",
    "    k = 1/hidden_size\n",
    "    for name, param in m.named_parameters():\n",
    "        if name in [\"pre_predictor.weight\",\"pre_predictor.bias\",\"predictor.weight\",\"predictor.bias\"]:\n",
    "            if 'weight' in name:\n",
    "                nn.init.uniform_(param.data, a=-1*k**0.5, b=k**0.5)\n",
    "            else:\n",
    "                nn.init.uniform_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_predictor.weight\n",
      "pre_predictor.bias\n",
      "predictor.weight\n",
      "predictor.bias\n",
      "Model Initialized\n"
     ]
    }
   ],
   "source": [
    "#define hyperparameters\n",
    "\n",
    "LR = 1e-4\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "#define models, move to device, and initialize weights\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = StabilityPredictor(esm=pretrainedESM).to(device)\n",
    "model.apply(init_classification_head_weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training function\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for databatch in dataloader:\n",
    "        input_ids, input_masks, stability = databatch['input_ids'], databatch['input_mask'], databatch['targets']\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input_ids.to(device))\n",
    "        loss = F.mse_loss(pred, stability.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss/len(dataloader)\n",
    "        \n",
    "        \n",
    "\n",
    "#define evaluation function\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for databatch in dataloader:\n",
    "            input_ids, input_masks, stability = databatch['input_ids'], databatch['input_mask'], databatch['targets']\n",
    "            pred = model(input_ids.to(device))\n",
    "            loss = F.mse_loss(pred, stability.to(device))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_855147/1590672328.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  token_ids = torch.from_numpy(pad_sequences(np.array(token_ids), 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Train Loss: 0.329\n",
      "Initial Valid Loss: 0.466\n",
      "\tTrain Loss: 0.207\n",
      "\tValid Loss: 0.462\n",
      "\tTrain Loss: 0.168\n",
      "\tValid Loss: 0.338\n",
      "\tTrain Loss: 0.141\n",
      "\tValid Loss: 0.341\n",
      "\tTrain Loss: 0.119\n",
      "\tValid Loss: 0.372\n",
      "\tTrain Loss: 0.104\n",
      "\tValid Loss: 0.159\n",
      "\tTrain Loss: 0.083\n",
      "\tValid Loss: 0.171\n",
      "\tTrain Loss: 0.067\n",
      "\tValid Loss: 0.150\n",
      "\tTrain Loss: 0.057\n",
      "\tValid Loss: 0.178\n",
      "\tTrain Loss: 0.052\n",
      "\tValid Loss: 0.267\n",
      "\tTrain Loss: 0.046\n",
      "\tValid Loss: 0.202\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_loss = evaluate(model, train_dataloader, device)\n",
    "valid_loss = evaluate(model, val_dataloader, device)\n",
    "\n",
    "print(f'Initial Train Loss: {train_loss:.3f}')\n",
    "print(f'Initial Valid Loss: {valid_loss:.3f}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss = train(model, train_dataloader, optimizer, device)\n",
    "    valid_loss = evaluate(model, val_dataloader, device)\n",
    "    \n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torcheval.metrics.functional import r2_score\n",
    "from torchmetrics import SpearmanCorrCoef\n",
    "\n",
    "\n",
    "def check_metrices(pred, gold):\n",
    "    model.eval()\n",
    "    gold = gold\n",
    "    mse = F.mse_loss(pred, gold) \n",
    "    rmse = torch.sqrt(mse)\n",
    "    r_square = r2_score(pred, gold)\n",
    "    cal_spearman = SpearmanCorrCoef()\n",
    "    spearman = cal_spearman(pred[:,0], gold[:,0])\n",
    "    print(f'mse:{mse}, rmse:{rmse}, r_square:{r_square}, spearman:{spearman}')\n",
    "    \n",
    "\n",
    "\n",
    "# predict the whole dataset\n",
    "\n",
    "def make_predictions(model, dataloader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.cpu()\n",
    "        predictions = torch.tensor([])\n",
    "        golds = torch.tensor([])\n",
    "        for databatch in dataloader:\n",
    "            input_ids, input_masks, stability = databatch['input_ids'], databatch['input_mask'], databatch['targets']\n",
    "            pred = model(input_ids)\n",
    "            predictions = torch.cat((predictions, pred), dim=0)\n",
    "            golds = torch.cat((golds, stability), dim=0)\n",
    "    return predictions, golds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_855147/1590672328.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  token_ids = torch.from_numpy(pad_sequences(np.array(token_ids), 0))\n"
     ]
    }
   ],
   "source": [
    "#get predictions \n",
    "\n",
    "train_preds, train_golds = make_predictions(model, train_dataloader)\n",
    "val_preds, val_golds = make_predictions(model, val_dataloader)\n",
    "test_preds, test_golds = make_predictions(model, test_dataloader)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrices: -----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirali/zoey/env/dlEnv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.11966745555400848, rmse:0.3459298312664032, r_square:0.6267509460449219, spearman:0.860129177570343\n",
      "None\n",
      "validation metrices: -----------------------------------------------------------\n",
      "mse:0.20187589526176453, rmse:0.44930601119995117, r_square:0.5303531885147095, spearman:0.7541908621788025\n",
      "None\n",
      "test metrices: -----------------------------------------------------------\n",
      "mse:0.250195175409317, rmse:0.5001951456069946, r_square:-0.49777042865753174, spearman:0.6967188715934753\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#print the metrices\n",
    "\n",
    "\n",
    "print(f'train metrices: -----------------------------------------------------------')\n",
    "\n",
    "print(check_metrices(train_preds, train_golds))\n",
    "\n",
    "print(f'validation metrices: -----------------------------------------------------------')\n",
    "\n",
    "print(check_metrices(val_preds, val_golds))\n",
    "\n",
    "print(f'test metrices: -----------------------------------------------------------')\n",
    "\n",
    "print(check_metrices(test_preds, test_golds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dlEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c370240dcf75e4c427ae0409725d172688b9503e127871abb71af5f78d70e71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
