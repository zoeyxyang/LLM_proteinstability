{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ProteinBERT model trained on TAPE Stability Dataset\n",
        "\n",
        "## By Aryan Vats"
      ],
      "metadata": {
        "id": "sYvmg_7P_A4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "B3FGr0d2-9r3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia7eeuk01A3w",
        "outputId": "9366b44f-9703-494f-fc5e-cb671ab82c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-27 17:07:14--  http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/stability.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.90.86, 54.231.136.192, 52.217.121.88, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.90.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3116829 (3.0M) [application/x-tar]\n",
            "Saving to: ‘stability.tar.gz’\n",
            "\n",
            "stability.tar.gz    100%[===================>]   2.97M  2.19MB/s    in 1.4s    \n",
            "\n",
            "2023-04-27 17:07:16 (2.19 MB/s) - ‘stability.tar.gz’ saved [3116829/3116829]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -N http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/stability.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEP3uCp8ImvF",
        "outputId": "be17f226-45b2-4512-9637-1fada7b02a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tape_proteins\n",
            "  Downloading tape_proteins-0.5-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tape_proteins) (2.27.1)\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tape_proteins) (1.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from tape_proteins) (3.12.0)\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tape_proteins) (4.65.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.121-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython->tape_proteins) (1.22.4)\n",
            "Collecting botocore<1.30.0,>=1.29.121\n",
            "  Downloading botocore-1.29.121-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tape_proteins) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tape_proteins) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->tape_proteins) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tape_proteins) (3.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->tape_proteins) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX->tape_proteins) (23.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.121->boto3->tape_proteins) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.121->boto3->tape_proteins) (1.16.0)\n",
            "Installing collected packages: lmdb, tensorboardX, jmespath, biopython, botocore, s3transfer, boto3, tape_proteins\n",
            "Successfully installed biopython-1.81 boto3-1.26.121 botocore-1.29.121 jmespath-1.0.1 lmdb-1.4.1 s3transfer-0.6.0 tape_proteins-0.5 tensorboardX-2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install tape_proteins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrAATSJ-6GF0",
        "outputId": "cd5fc3da-8633-4c63-d9b1-f9de58802fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stability/\n",
            "stability/stability_test.lmdb/\n",
            "stability/stability_test.lmdb/data.mdb\n",
            "stability/stability_test.lmdb/lock.mdb\n",
            "stability/stability_train.lmdb/\n",
            "stability/stability_train.lmdb/data.mdb\n",
            "stability/stability_train.lmdb/lock.mdb\n",
            "stability/stability_valid.lmdb/\n",
            "stability/stability_valid.lmdb/data.mdb\n",
            "stability/stability_valid.lmdb/lock.mdb\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protein-bert\n",
            "  Downloading protein_bert-1.0.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from protein-bert) (1.22.4)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from protein-bert) (4.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from protein-bert) (1.5.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from protein-bert) (3.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from protein-bert) (2.12.0)\n",
            "Collecting pyfaidx\n",
            "  Downloading pyfaidx-0.7.2.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->protein-bert) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->protein-bert) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyfaidx->protein-bert) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyfaidx->protein-bert) (67.7.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.54.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (16.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.12.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (23.3.3)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (0.4.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (3.20.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (2.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->protein-bert) (1.6.3)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->protein-bert) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->protein-bert) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow->protein-bert) (0.1.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->protein-bert) (1.8.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->protein-bert) (3.2.2)\n",
            "Installing collected packages: typeguard, pyfaidx, tensorflow-addons, protein-bert\n",
            "Successfully installed protein-bert-1.0.1 pyfaidx-0.7.2.1 tensorflow-addons-0.20.0 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.6-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtnt>=0.0.5\n",
            "  Downloading torchtnt-0.0.7-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2023.4.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2.12.2)\n",
            "Collecting pyre-extensions\n",
            "  Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (23.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2.0.0+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (5.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (1.22.4)\n",
            "Collecting typing-inspect\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.4.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.4.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.40.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.27.1)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.20.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.0.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (16.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (5.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->torchtnt>=0.0.5->torcheval) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtnt>=0.0.5->torcheval) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (3.2.2)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, torchtnt, torcheval\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.30 torcheval-0.0.6 torchtnt-0.0.7 typing-inspect-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!tar -xzvf /content/stability.tar.gz -C /content/data\n",
        "!pip install lmdb\n",
        "!pip install protein-bert\n",
        "!pip install torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdOgdZBc-GSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71afcac-e857-4fe9-c2fe-5bb1eaff5058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-27 17:08:06--  http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/stability.tar.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.40.118, 52.217.161.56, 52.216.36.72, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.40.118|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1834526 (1.7M) [application/x-tar]\n",
            "Saving to: ‘stability.tar.gz.1’\n",
            "\n",
            "stability.tar.gz.1  100%[===================>]   1.75M  1.30MB/s    in 1.3s    \n",
            "\n",
            "2023-04-27 17:08:08 (1.30 MB/s) - ‘stability.tar.gz.1’ saved [1834526/1834526]\n",
            "\n",
            "stability/stability_test.json\n",
            "stability/stability_train.json\n",
            "stability/stability_valid.json\n"
          ]
        }
      ],
      "source": [
        "!wget http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/stability.tar.gz\n",
        "!mkdir raw2\n",
        "!tar -xzvf /content/stability.tar.gz.1 -C /content/raw2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjR-L__v_I1n",
        "outputId": "48312df8-b2c6-44d2-b204-2e192dbedabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-27 17:08:08--  ftp://ftp.cs.huji.ac.il/users/nadavb/protein_bert/epoch_92400_sample_23500000.pkl\n",
            "           => ‘/content/proteinbertweights/epoch_92400_sample_23500000.pkl’\n",
            "Resolving ftp.cs.huji.ac.il (ftp.cs.huji.ac.il)... 132.65.116.15\n",
            "Connecting to ftp.cs.huji.ac.il (ftp.cs.huji.ac.il)|132.65.116.15|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /users/nadavb/protein_bert ... done.\n",
            "==> SIZE epoch_92400_sample_23500000.pkl ... 191800918\n",
            "==> PASV ... done.    ==> RETR epoch_92400_sample_23500000.pkl ... done.\n",
            "Length: 191800918 (183M) (unauthoritative)\n",
            "\n",
            "epoch_92400_sample_ 100%[===================>] 182.92M  3.60MB/s    in 93s     \n",
            "\n",
            "2023-04-27 17:09:47 (1.97 MB/s) - ‘/content/proteinbertweights/epoch_92400_sample_23500000.pkl’ saved [191800918]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget ftp://ftp.cs.huji.ac.il/users/nadavb/protein_bert/epoch_92400_sample_23500000.pkl -P /content/proteinbertweights\n",
        "\n",
        "\n",
        "PBDIR = '/content/proteinbertweights/'\n",
        "PBNAME = 'epoch_92400_sample_23500000.pkl'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "jJTrUaPk-7L8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1za1uzWi2Nw0"
      },
      "outputs": [],
      "source": [
        "# ProteinBert Dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
        "from proteinbert.existing_model_loading import load_pretrained_model_from_dump\n",
        "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs,create_model\n",
        "\n",
        "\n",
        "# Transfer Learning Dependencies\n",
        "from tensorflow import keras\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "\n",
        "# LMDB Dataset Load Dependencies\n",
        "\n",
        "from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection\n",
        "from copy import copy\n",
        "from pathlib import Path\n",
        "import pickle as pkl\n",
        "import logging\n",
        "import random\n",
        "import lmdb\n",
        "import numpy as np\n",
        "import torch\n",
        "from torcheval.metrics.functional import r2_score,mean_squared_error,root\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from tape import TAPETokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))"
      ],
      "metadata": {
        "id": "rHZWpL8TvG7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the data pipeline according to the TAPE repository"
      ],
      "metadata": {
        "id": "q6Rap9j_-1P4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRRwbjs72Nll"
      },
      "outputs": [],
      "source": [
        "class LMDBDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from an lmdb file.\n",
        "    Args:\n",
        "        data_file (Union[str, Path]): Path to lmdb file.\n",
        "        in_memory (bool, optional): Whether to load the full dataset into memory.\n",
        "            Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_file: Union[str, Path],\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        data_file = Path(data_file)\n",
        "        if not data_file.exists():\n",
        "            raise FileNotFoundError(data_file)\n",
        "\n",
        "        env = lmdb.open(str(data_file), max_readers=1, readonly=True,\n",
        "                        lock=False, readahead=False, meminit=False)\n",
        "\n",
        "        with env.begin(write=False) as txn:\n",
        "            num_examples = pkl.loads(txn.get(b'num_examples'))\n",
        "\n",
        "        if in_memory:\n",
        "            cache = [None] * num_examples\n",
        "            self._cache = cache\n",
        "\n",
        "        self._env = env\n",
        "        self._in_memory = in_memory\n",
        "        self._num_examples = num_examples\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self._num_examples\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        if not 0 <= index < self._num_examples:\n",
        "            raise IndexError(index)\n",
        "\n",
        "        if self._in_memory and self._cache[index] is not None:\n",
        "            item = self._cache[index]\n",
        "        else:\n",
        "            with self._env.begin(write=False) as txn:\n",
        "                item = pkl.loads(txn.get(str(index).encode()))\n",
        "                if 'id' not in item:\n",
        "                    item['id'] = str(index)\n",
        "                if self._in_memory:\n",
        "                    self._cache[index] = item\n",
        "        return item\n",
        "\n",
        "\n",
        "def dataset_factory(data_file: Union[str, Path], *args, **kwargs) -> Dataset:\n",
        "    data_file = Path(data_file)\n",
        "    if not data_file.exists():\n",
        "        raise FileNotFoundError(data_file)\n",
        "    if data_file.suffix == '.lmdb':\n",
        "        return LMDBDataset(data_file, *args, **kwargs)\n",
        "    # elif data_file.suffix in {'.fasta', '.fna', '.ffn', '.faa', '.frn'}:\n",
        "    #     return FastaDataset(data_file, *args, **kwargs)\n",
        "    # elif data_file.suffix == '.json':\n",
        "    #     return JSONDataset(data_file, *args, **kwargs)\n",
        "    # elif data_file.is_dir():\n",
        "    #     return NPZDataset(data_file, *args, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unrecognized datafile type {data_file.suffix}\")\n",
        "\n",
        "\n",
        "def pad_sequences(sequences: Sequence, constant_value=0, dtype=None) -> np.ndarray:\n",
        "    batch_size = len(sequences)\n",
        "    shape = [batch_size] + np.max([seq.shape for seq in sequences], 0).tolist()\n",
        "\n",
        "    if dtype is None:\n",
        "        dtype = sequences[0].dtype\n",
        "\n",
        "    if isinstance(sequences[0], np.ndarray):\n",
        "        array = np.full(shape, constant_value, dtype=dtype)\n",
        "    elif isinstance(sequences[0], torch.Tensor):\n",
        "        array = torch.full(shape, constant_value, dtype=dtype)\n",
        "\n",
        "    for arr, seq in zip(array, sequences):\n",
        "        arrslice = tuple(slice(dim) for dim in seq.shape)\n",
        "        arr[arrslice] = seq\n",
        "\n",
        "    return array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSUIezvk-dVY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class StabilityDataset(Dataset):\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_path: Union[str, Path],\n",
        "                 split: str,\n",
        "                 tokenizer: Union[str, TAPETokenizer] = 'iupac',\n",
        "                 in_memory: bool = False):\n",
        "\n",
        "        if split not in ('train', 'valid', 'test'):\n",
        "            raise ValueError(f\"Unrecognized split: {split}. \"\n",
        "                             f\"Must be one of ['train', 'valid', 'test']\")\n",
        "        if isinstance(tokenizer, str):\n",
        "            tokenizer = TAPETokenizer(vocab=tokenizer)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data_path = Path(data_path)\n",
        "        data_file = f'stability/stability_{split}.lmdb'\n",
        "\n",
        "        self.data = dataset_factory(data_path / data_file, in_memory)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        item = self.data[index]\n",
        "        token_ids = self.tokenizer.encode(item['primary'])\n",
        "        input_mask = np.ones_like(token_ids)\n",
        "        # prim = item['prim']\n",
        "        return token_ids, input_mask, float(item['stability_score'][0]),item['primary']\n",
        "\n",
        "    def get_prim_data(self, batch: List[Tuple[Any, ...]]):\n",
        "        \n",
        "        input_ids, input_mask, stability_true_value, primary = tuple(zip(*batch))\n",
        "        \n",
        "        # stability_true_value = torch.FloatTensor(stability_true_value)  # type: ignore\n",
        "        # stability_true_value = stability_true_value.unsqueeze(1)\n",
        "        return {\n",
        "            'seq' : primary,\n",
        "            'label' : stability_true_value\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Any, ...]]) -> Dict[str, torch.Tensor]:\n",
        "        # print(*batch)\n",
        "        input_ids, input_mask, stability_true_value,_ = tuple(zip(*batch))\n",
        "        input_ids = torch.from_numpy(pad_sequences(input_ids, 0))\n",
        "        input_mask = torch.from_numpy(pad_sequences(input_mask, 0))\n",
        "        stability_true_value = torch.FloatTensor(stability_true_value)  # type: ignore\n",
        "        stability_true_value = stability_true_value.unsqueeze(1)\n",
        "\n",
        "        return {'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'targets': stability_true_value}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Oehbahy4nbu"
      },
      "outputs": [],
      "source": [
        "train_data = StabilityDataset('/content/data/','train')\n",
        "test_data = StabilityDataset('/content/data/','test')\n",
        "valid_data = StabilityDataset('/content/data/','valid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziSLV3vxJ2Tj",
        "outputId": "a8d71e27-9943-4abb-a69d-4a59ee3230b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2 11 22 21 10  5  5 28 20 14  9 11  8  8  5 23 12 21  9  9 15 21  9  9\n",
            " 14 23 22 17 26  5 23 20  5  5  9 25 14 21 11  8 11 17 13 26  3] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1] 0.09000000357627869\n"
          ]
        }
      ],
      "source": [
        "item = train_data.data[560]\n",
        "token_ids = train_data.tokenizer.encode(item['primary'])\n",
        "input_mask = np.ones_like(token_ids)\n",
        "print(token_ids, input_mask, float(item['stability_score'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the model and dividing data accordingly "
      ],
      "metadata": {
        "id": "HLZmeKvE-wUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za7YJng858W2"
      },
      "outputs": [],
      "source": [
        "pretrained_model_generator, input_encoder = load_pretrained_model(PBDIR,PBNAME)\n",
        "\n",
        "\n",
        "get_model_with_hidden_layers_as_outputs gives the model output access to the hidden layers (on top of the output)\n",
        "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
        "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_ImipFO8eWU"
      },
      "outputs": [],
      "source": [
        "g = train_data.data[0]\n",
        "# print(g)\n",
        "train_batch = train_data.get_prim_data(train_data)\n",
        "test_batch = test_data.get_prim_data(test_data)\n",
        "valid_batch = valid_data.get_prim_data(valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbU2cg2G9SBc"
      },
      "outputs": [],
      "source": [
        "train_set = pd.DataFrame.from_dict(train_batch)\n",
        "valid_set = pd.DataFrame.from_dict(valid_batch)\n",
        "test_set = pd.DataFrame.from_dict(test_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7vmZehkHMhr",
        "outputId": "10e13c98-44a5-4a49-fd6a-8c0e6b7d0f5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0.17\n",
              "1       -0.18\n",
              "2        1.36\n",
              "3        0.65\n",
              "4       -0.47\n",
              "         ... \n",
              "53609    1.05\n",
              "53610    0.83\n",
              "53611    1.10\n",
              "53612    1.19\n",
              "53613    1.00\n",
              "Name: label, Length: 53614, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "train_set['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "UI8sOwVu9uW8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpcY_Lrf7RgC"
      },
      "outputs": [],
      "source": [
        "OUTPUT_TYPE = OutputType(False, 'numeric')\n",
        "\n",
        "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE)\n",
        "\n",
        "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
        "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO5EleA_BXdL",
        "outputId": "550d21ba-5d76-4d48-ba6e-f5bf903b8bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023_04_19-01:29:21] Training set: Filtered out 0 of 53614 (0.0%) records of lengths exceeding 510.\n",
            "[2023_04_19-01:29:24] Validation set: Filtered out 0 of 2512 (0.0%) records of lengths exceeding 510.\n",
            "[2023_04_19-01:29:24] Training with frozen pretrained layers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "3351/3351 [==============================] - ETA: 0s - loss: 0.7226"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3351/3351 [==============================] - 4474s 1s/step - loss: 0.7226 - val_loss: 0.3156 - lr: 0.0100\n",
            "Epoch 2/3\n",
            " 757/3351 [=====>........................] - ETA: 51:37 - loss: 0.7131"
          ]
        }
      ],
      "source": [
        "\n",
        "training_callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
        "    keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True),\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \\\n",
        "        seq_len = 512, batch_size = 16, max_epochs_per_stage = 3, lr = 1e-04, begin_with_frozen_pretrained_layers = True, \\\n",
        "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = 1e-05, callbacks = training_callbacks)\n",
        "\n",
        "\n",
        "f = open('weights.pkl','w')\n",
        "pickle.dump(model_generator.model_weights,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reloading model from saved weights to avoid rerunning"
      ],
      "metadata": {
        "id": "u8PW6rEL9mNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = pickle.load(open('weights.pkl','rb'))\n",
        "\n",
        "\n",
        "model_generator_new = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
        "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
        "model_generator_new.model_weights = weights\n",
        "\n"
      ],
      "metadata": {
        "id": "MJrVUe5pMPf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Set Loss"
      ],
      "metadata": {
        "id": "N7vdq3j69kOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Referenced from Prot-BERT functions and modified to yield MSE, R SQUARE, Spearman's Rho\n",
        "\n",
        "\n",
        "def encode_dataset(seqs, raw_Y, input_encoder, output_spec, seq_len = 512, needs_filtering = True, dataset_name = 'Dataset', verbose = True):\n",
        "    \n",
        "    if needs_filtering:\n",
        "        dataset = pd.DataFrame({'seq': seqs, 'raw_Y': raw_Y})\n",
        "        dataset = filter_dataset_by_len(dataset, seq_len = seq_len, dataset_name = dataset_name, verbose = verbose)\n",
        "        seqs = dataset['seq']\n",
        "        raw_Y = dataset['raw_Y']\n",
        "    \n",
        "    X = input_encoder.encode_X(seqs, seq_len)\n",
        "    Y, sample_weigths = encode_Y(raw_Y, output_spec, seq_len = seq_len)\n",
        "    return X, Y, sample_weigths\n",
        "\n",
        "def encode_Y(raw_Y, output_spec, seq_len = 512):\n",
        "    if output_spec.output_type.is_seq:\n",
        "        return encode_seq_Y(raw_Y, seq_len, output_spec.output_type.is_binary, output_spec.unique_labels)\n",
        "    elif output_spec.output_type.is_categorical:\n",
        "        return encode_categorical_Y(raw_Y, output_spec.unique_labels), np.ones(len(raw_Y))\n",
        "    elif output_spec.output_type.is_numeric or output_spec.output_type.is_binary:\n",
        "        return raw_Y.values.astype(float), np.ones(len(raw_Y))\n",
        "    else:\n",
        "        raise ValueError('Unexpected output type: %s' % output_spec.output_type)\n",
        "\n",
        "def encode_seq_Y(seqs, seq_len, is_binary, unique_labels):\n",
        "\n",
        "    label_to_index = {str(label): i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "    Y = np.zeros((len(seqs), seq_len), dtype = int)\n",
        "    sample_weigths = np.zeros((len(seqs), seq_len))\n",
        "    \n",
        "    for i, seq in enumerate(seqs):\n",
        "        \n",
        "        for j, label in enumerate(seq):\n",
        "            # +1 to account for the <START> token at the beginning.\n",
        "            Y[i, j + 1] = label_to_index[label]\n",
        "            \n",
        "        sample_weigths[i, 1:(len(seq) + 1)] = 1\n",
        "        \n",
        "    if is_binary:\n",
        "        Y = np.expand_dims(Y, axis = -1)\n",
        "        sample_weigths = np.expand_dims(sample_weigths, axis = -1)\n",
        "    \n",
        "    return Y, sample_weigths\n",
        "\n",
        "\n",
        "def encode_dataset(seqs, raw_Y, input_encoder, output_spec, seq_len = 512, needs_filtering = True, dataset_name = 'Dataset', verbose = True):\n",
        "    \n",
        "    if needs_filtering:\n",
        "        dataset = pd.DataFrame({'seq': seqs, 'raw_Y': raw_Y})\n",
        "        dataset = filter_dataset_by_len(dataset, seq_len = seq_len, dataset_name = dataset_name, verbose = verbose)\n",
        "        seqs = dataset['seq']\n",
        "        raw_Y = dataset['raw_Y']\n",
        "    \n",
        "    X = input_encoder.encode_X(seqs, seq_len)\n",
        "    Y, sample_weigths = encode_Y(raw_Y, output_spec, seq_len = seq_len)\n",
        "    return X, Y, sample_weigths\n",
        "\n",
        "def encode_Y(raw_Y, output_spec, seq_len = 512):\n",
        "    if output_spec.output_type.is_seq:\n",
        "        return encode_seq_Y(raw_Y, seq_len, output_spec.output_type.is_binary, output_spec.unique_labels)\n",
        "    # elif output_spec.output_type.is_categorical:\n",
        "    #     return encode_categorical_Y(raw_Y, output_spec.unique_labels), np.ones(len(raw_Y))\n",
        "    elif output_spec.output_type.is_numeric or output_spec.output_type.is_binary:\n",
        "        return raw_Y.values.astype(float), np.ones(len(raw_Y))\n",
        "    else:\n",
        "        raise ValueError('Unexpected output type: %s' % output_spec.output_type)\n",
        "\n",
        "def encode_seq_Y(seqs, seq_len, is_binary, unique_labels):\n",
        "\n",
        "    label_to_index = {str(label): i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "    Y = np.zeros((len(seqs), seq_len), dtype = int)\n",
        "    sample_weigths = np.zeros((len(seqs), seq_len))\n",
        "    \n",
        "    for i, seq in enumerate(seqs):\n",
        "        \n",
        "        for j, label in enumerate(seq):\n",
        "            # +1 to account for the <START> token at the beginning.\n",
        "            Y[i, j + 1] = label_to_index[label]\n",
        "            \n",
        "        sample_weigths[i, 1:(len(seq) + 1)] = 1\n",
        "        \n",
        "    if is_binary:\n",
        "        Y = np.expand_dims(Y, axis = -1)\n",
        "        sample_weigths = np.expand_dims(sample_weigths, axis = -1)\n",
        "    \n",
        "    return Y, sample_weigths\n",
        "\n",
        "def split_dataset_by_len(dataset, seq_col_name = 'seq', start_seq_len = 512, start_batch_size = 32, increase_factor = 2):\n",
        "\n",
        "    seq_len = start_seq_len\n",
        "    batch_size = start_batch_size\n",
        "    \n",
        "    while len(dataset) > 0:\n",
        "        max_allowed_input_seq_len = seq_len - 2\n",
        "        len_mask = (dataset[seq_col_name].str.len() <= max_allowed_input_seq_len)\n",
        "        len_matching_dataset = dataset[len_mask]\n",
        "        yield len_matching_dataset, seq_len, batch_size\n",
        "        dataset = dataset[~len_mask]\n",
        "        seq_len *= increase_factor\n",
        "        batch_size = max(batch_size // increase_factor, 1)\n",
        "\n",
        "        # y_true = y_true[y_mask].flatten()\n",
        "        # y_pred = y_pred[y_mask]\n",
        "\n",
        "\n",
        "dataset = pd.DataFrame({'seq': test_set['seq'], 'raw_y': test_set['label']})\n",
        "    \n",
        "results = []\n",
        "results_names = []\n",
        "y_trues = []\n",
        "y_preds = []\n",
        "\n",
        "\n",
        "\n",
        "for len_matching_dataset, seq_len, batch_size in split_dataset_by_len(dataset, start_seq_len = 512, start_batch_size = 32, \\\n",
        "        increase_factor = 2):\n",
        "\n",
        "    X, y_true, sample_weights = encode_dataset(len_matching_dataset['seq'], len_matching_dataset['raw_y'], input_encoder, OUTPUT_SPEC, \\\n",
        "            seq_len = seq_len, needs_filtering = False)\n",
        "    \n",
        "    y_mask = (sample_weights == 1)\n",
        "\n",
        "    # print(X)\n",
        "    # print(y_true)\n",
        "    mod = model_generator_new.create_model(seq_len)\n",
        "    y_pred = mod.predict(X,batch_size = batch_size)\n",
        "\n",
        "    \n",
        "    # print(y_pred)\n",
        "    # print(y_true)\n",
        "    # print(y_mask)\n",
        "    # print(np.array(y_pred).shape)\n",
        "    y_true = y_true[y_mask].flatten()\n",
        "    y_pred = y_pred[y_mask]\n",
        "\n",
        "    if OUTPUT_SPEC.output_type.is_categorical:\n",
        "        y_pred = y_pred.reshape((-1, y_pred.shape[-1]))\n",
        "    else:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # results.append(get_evaluation_results(y_true, y_pred, output_spec))\n",
        "    # results_names.append(seq_len)\n",
        "\n",
        "    y_trues.append(y_true)\n",
        "    y_preds.append(y_pred)\n",
        "\n",
        "    \n",
        "y_t = np.squeeze(np.array(y_trues))\n",
        "y_p = np.squeeze(np.array(y_preds))\n",
        "\n",
        "y_t_ten = torch.tensor(y_t)\n",
        "y_p_ten = torch.tensor(y_p)\n",
        "# print(y_t_ten)\n",
        "print(\"Test Set Losses\")\n",
        "print(\"R2 Squared : \",r2_score(y_t_ten,y_p_ten).item())\n",
        "print(\"Mean Squared Error: \",mean_squared_error(y_p_ten,y_t_ten).item())\n",
        "print(\"Root Mean Squared Error: \",RMSELoss(y_p_ten,y_t_ten))\n",
        "print(\"Spearman's Rho : \",spearmanr(y_t,y_p)[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5bkL3ccXFxK",
        "outputId": "accf8316-5e91-4ca9-90c3-2e3a61f3773c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "402/402 [==============================] - 1073s 3s/step\n",
            "Test Set Losses\n",
            "R2 Squared :  0.041116122626293605\n",
            "Mean Squared Error:  0.18469268890822468\n",
            "Root Mean Squared Error:  tensor(0.4298, dtype=torch.float64)\n",
            "Spearman's Rho :  0.6001859931751315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Set Loss"
      ],
      "metadata": {
        "id": "OeMlVIYX9ghB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = pd.DataFrame({'seq': valid_set['seq'], 'raw_y': valid_set['label']})\n",
        "    \n",
        "results = []\n",
        "results_names = []\n",
        "y_trues = []\n",
        "y_preds = []\n",
        "\n",
        "\n",
        "\n",
        "for len_matching_dataset, seq_len, batch_size in split_dataset_by_len(dataset, start_seq_len = 512, start_batch_size = 32, \\\n",
        "        increase_factor = 2):\n",
        "\n",
        "    X, y_true, sample_weights = encode_dataset(len_matching_dataset['seq'], len_matching_dataset['raw_y'], input_encoder, OUTPUT_SPEC, \\\n",
        "            seq_len = seq_len, needs_filtering = False)\n",
        "    \n",
        "    y_mask = (sample_weights == 1)\n",
        "\n",
        "    # print(X)\n",
        "    # print(y_true)\n",
        "    mod = model_generator_new.create_model(seq_len)\n",
        "    y_pred = mod.predict(X,batch_size = batch_size)\n",
        "\n",
        "    \n",
        "    # print(y_pred)\n",
        "    # print(y_true)\n",
        "    # print(y_mask)\n",
        "    # print(np.array(y_pred).shape)\n",
        "    y_true = y_true[y_mask].flatten()\n",
        "    y_pred = y_pred[y_mask]\n",
        "\n",
        "    if OUTPUT_SPEC.output_type.is_categorical:\n",
        "        y_pred = y_pred.reshape((-1, y_pred.shape[-1]))\n",
        "    else:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # results.append(get_evaluation_results(y_true, y_pred, output_spec))\n",
        "    # results_names.append(seq_len)\n",
        "\n",
        "    y_trues.append(y_true)\n",
        "    y_preds.append(y_pred)\n",
        "\n",
        "y_t = np.squeeze(np.array(y_trues))\n",
        "y_p = np.squeeze(np.array(y_preds))\n",
        "\n",
        "y_t_ten = torch.tensor(y_t)\n",
        "y_p_ten = torch.tensor(y_p)\n",
        "print(\"Valid Set Losses\")\n",
        "print(\"R2 Squared : \",r2_score(y_p_ten,y_t_ten).item())\n",
        "print(\"Mean Squared Error: \",mean_squared_error(y_p_ten,y_t_ten).item())\n",
        "print(\"Root Mean Squared Error: \",RMSELoss(y_p_ten,y_t_ten))\n",
        "print(\"Spearman's Rho : \",spearmanr(y_p,y_t)[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSHDTsT3ssWd",
        "outputId": "640f7c8f-1fa1-4367-defe-a805c9f90fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Set Losses\n",
            "R2 Squared :  0.38492534475452744\n",
            "Mean Squared Error:  0.26438749936646905\n",
            "Root Mean Squared Error:  tensor(0.5142, dtype=torch.float64)\n",
            "Spearman's Rho :  0.601912958753572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Set Loss "
      ],
      "metadata": {
        "id": "H1t3r9wM9UYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = pd.DataFrame({'seq': train_set['seq'], 'raw_y': train_set['label']})\n",
        "    \n",
        "results = []\n",
        "results_names = []\n",
        "y_trues = []\n",
        "y_preds = []\n",
        "\n",
        "\n",
        "\n",
        "for len_matching_dataset, seq_len, batch_size in split_dataset_by_len(dataset, start_seq_len = 512, start_batch_size = 32, \\\n",
        "        increase_factor = 2):\n",
        "\n",
        "    X, y_true, sample_weights = encode_dataset(len_matching_dataset['seq'], len_matching_dataset['raw_y'], input_encoder, OUTPUT_SPEC, \\\n",
        "            seq_len = seq_len, needs_filtering = False)\n",
        "    \n",
        "    y_mask = (sample_weights == 1)\n",
        "\n",
        "    # print(X)\n",
        "    # print(y_true)\n",
        "    mod = model_generator_new.create_model(seq_len)\n",
        "    y_pred = mod.predict(X,batch_size = batch_size)\n",
        "\n",
        "    \n",
        "    # print(y_pred)\n",
        "    # print(y_true)\n",
        "    # print(y_mask)\n",
        "    # print(np.array(y_pred).shape)\n",
        "    y_true = y_true[y_mask].flatten()\n",
        "    y_pred = y_pred[y_mask]\n",
        "\n",
        "    if OUTPUT_SPEC.output_type.is_categorical:\n",
        "        y_pred = y_pred.reshape((-1, y_pred.shape[-1]))\n",
        "    else:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # results.append(get_evaluation_results(y_true, y_pred, output_spec))\n",
        "    # results_names.append(seq_len)\n",
        "\n",
        "    y_trues.append(y_true)\n",
        "    y_preds.append(y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20f1qlaBssL0",
        "outputId": "4f3115e5-4dbc-4b12-896d-30d9408eb638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1676/1676 [==============================] - 4225s 3s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_t = np.squeeze(np.array(y_trues))\n",
        "y_p = np.squeeze(np.array(y_preds))\n",
        "\n",
        "y_t_ten = torch.tensor(y_t)\n",
        "y_p_ten = torch.tensor(y_p)\n",
        "# print(y_t_ten)\n",
        "print(\"R2 Squared : \",r2_score(y_p_ten,y_t_ten).item())\n",
        "print(\"Mean Squared Error: \",mean_squared_error(y_p_ten,y_t_ten).item())\n",
        "print(\"Root Mean Squared Error: \",RMSELoss(y_p_ten,y_t_ten))\n",
        "print(\"Spearman's Rho : \",spearmanr(y_t,y_p)[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJR2yRT4rF8S",
        "outputId": "544ddeee-94eb-45f6-8b52-a6de3865cf94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Squared :  0.2819435670937529\n",
            "Mean Squared Error:  0.23021622425403096\n",
            "Root Mean Squared Error:  tensor(0.4798, dtype=torch.float64)\n",
            "Spearman's Rho :  0.4357860097082521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Jb6LE3_PQ-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}